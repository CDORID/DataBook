{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regrouping knowledge in an automatized way for binary classification\n",
    "\n",
    "\n",
    "Starting point:\n",
    "X and Y are formatted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import os\n",
    "import matplotlib as plt\n",
    "\n",
    "import sklearn as sk\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import xgboost as xgb \n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/titanic/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the inner join of sales and controls\n",
    "\n",
    "\n",
    "X = df.drop('Survived', axis =1)\n",
    "Y = df['Survived']\n",
    "hypo = X.columns\n",
    "test = Y.columns\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalance:\n",
    "#### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous split : \n",
      "0 :  415 \n",
      "1 :  253\n",
      "New split : \n",
      "0 :  200 \n",
      "1 :  253\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def undersample(X_train,Y_train, target_var = str, minor_class = 1, limit_sample = 200):\n",
    "    \"\"\"\n",
    "    Simple undersampling, test keep the same proportion\n",
    "    \n",
    "    \"\"\"\n",
    "    # Rebuild training set\n",
    "    training_set = pd.concat([X_train,Y_train],axis = 1)\n",
    "    \n",
    "    # Choose major class, and minor class as 1 and 0s\n",
    "    major_class = 1-minor_class\n",
    "    print(\"Previous split : \\n0 : \",training_set[training_set[target_var] == 0].shape[0],\"\\n1 : \",\n",
    "          training_set[training_set[target_var] == 1].shape[0])\n",
    "    \n",
    "    \n",
    "    #Define which values will be removed randomly\n",
    "    major_indices  = training_set[training_set[target_var] == major_class].index\n",
    "    minor_indices = training_set[training_set[target_var] == minor_class].index\n",
    "    random_indices = np.random.choice(major_indices, limit_sample, replace=False)\n",
    "    kept_indices = np.sort(np.append(minor_indices,random_indices))\n",
    "\n",
    "                             \n",
    "    #Redefine the training set according to the undersampling\n",
    "    training_set_undersampled  = training_set.loc[kept_indices]\n",
    "\n",
    "\n",
    "    print(\"New split : \\n0 : \",training_set_undersampled[training_set_undersampled[target_var] == 0].shape[0],\"\\n1 : \",\n",
    "          training_set_undersampled[training_set_undersampled[target_var] == 1].shape[0])\n",
    "    \n",
    "\n",
    "    training_set_undersampled\n",
    "    X_train_under = training_set_undersampled.drop(target_var, axis =1)\n",
    "    Y_train_under = training_set_undersampled[target_var]\n",
    "    \n",
    "    return X_train_under, Y_train_under\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous split : \n",
      "0 :  415 \n",
      "1 :  253\n",
      "New split : \n",
      "0 :  200 \n",
      "1 :  253\n"
     ]
    }
   ],
   "source": [
    "X_trainu, Y_trainu = undersample(X_train,Y_train,\"Survived\",1,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import binclassfunc as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'S'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-5def16cc3c19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlog_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'balanced'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodelfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1527\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1528\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'S'"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state=0, class_weight = 'balanced')\n",
    "\n",
    "modelfit = log_reg.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.Model_KPI().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Models being tested \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "## log regs\n",
    "log_reg = LogisticRegression(random_state=0, class_weight = 'balanced')\n",
    "log_reg_elastic = LogisticRegression(class_weight = 'balanced',penalty= 'l1',solver = 'liblinear')\n",
    "log_reg_l1 = LogisticRegression(class_weight = 'balanced',penalty= 'elasticnet',solver= 'saga',l1_ratio=0.5) #saga solver works\n",
    "\n",
    "## svc\n",
    "svc_lin = svm.SVC(class_weight = 'balanced',kernel = 'linear',probability=True)\n",
    "svc_poly = svm.SVC(class_weight = 'balanced',kernel = 'poly',probability=True)\n",
    "svc_rbf = svm.SVC(class_weight = 'balanced',kernel = 'rbf',probability=True)\n",
    "svc_sigmoid = svm.SVC(class_weight = 'balanced',kernel = 'sigmoid',probability=True)\n",
    "\n",
    "## NB\n",
    "multi_NB = MultinomialNB()\n",
    "\n",
    "## Decision tree\n",
    "dec_tree = DecisionTreeClassifier(class_weight = 'balanced')\n",
    "xgboost = XGBClassifier(eval_metric ='auc'#scale_pos_weight = 5\n",
    "                       ) ###to be explained, its the ratio of imbalance\n",
    "\n",
    "## Ensemble \n",
    "adab = AdaBoostClassifier()\n",
    "random_forest = RandomForestClassifier(class_weight=\"balanced\")\n",
    "\n",
    "## LDA\n",
    "lda = LinearDiscriminantAnalysis() ##svd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def compute_AUC(model,X_test,Y_test):\n",
    "\n",
    "    yroc = Y_test.as_matrix().reshape(Y_test.as_matrix().size,1)\n",
    "    predroc = model.predict_proba(X_test)[:,1]\n",
    "    AUC = metrics.roc_auc_score(yroc, predroc)\n",
    "    return AUC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute ratios from confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "models = [log_reg,\n",
    "          log_reg_elastic,\n",
    "          log_reg_l1,\n",
    "          svc_lin,\n",
    "          svc_poly,\n",
    "          svc_rbf,\n",
    "          svc_sigmoid,\n",
    "          multi_NB,\n",
    "          dec_tree,\n",
    "          xgboost,\n",
    "          adab, #not adapted\n",
    "          random_forest,\n",
    "          lda      \n",
    "]\n",
    "\n",
    "undermodel = [multi_NB,\n",
    "             xgboost,\n",
    "             adab,\n",
    "             lda\n",
    "             ]\n",
    "\n",
    "\n",
    "\n",
    "CV = 4\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "fitted_models = []\n",
    "i = 0\n",
    "\n",
    "\n",
    "### Initializing dataframe to store kPIS\n",
    "\n",
    "Accuracy_df = pd.DataFrame({\"risk\":np.arange(0.0, 1.0, 0.005).tolist()})\n",
    "Precision_df = pd.DataFrame({\"risk\":np.arange(0.0, 1.0, 0.005).tolist()})\n",
    "Missclassified_1_df = pd.DataFrame({\"risk\":np.arange(0.0, 1.0, 0.005).tolist()})\n",
    "TNR_df = pd.DataFrame({\"risk\":np.arange(0.0, 1.0, 0.005).tolist()})\n",
    "TPR_df = pd.DataFrame({\"risk\":np.arange(0.0, 1.0, 0.005).tolist()})\n",
    "\n",
    "### initializing list for individual kpis\n",
    "AUC_list = []\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    ## report models \n",
    "    i = i+1\n",
    "    model_name = str(str(i) +\"-\"+ model.__class__.__name__)\n",
    "    print(model_name)\n",
    "    \n",
    "   \n",
    "    ### verify the data input on undersampling\n",
    "    if model in undermodel : \n",
    "        \n",
    "        ### We use the undersampled data\n",
    "        accuracies = cross_val_score(model, Xunder, Yunder, scoring='accuracy', cv=CV)\n",
    "        \n",
    "        fitted_model = model.fit(X_train_under,Y_train_under)\n",
    "        prediction_train = model.predict(X_train_under)\n",
    "        \n",
    "        ## compute KPIs\n",
    "        AUC = compute_AUC(model,X_test_under,Y_test_under)\n",
    "        Accuracy,Precision_df_1,Missclassified_1,TNR_df_1,TPR_df_1 = compute_KPIs(fitted_model,model_name,X_test_under,Y_test_under)\n",
    "        \n",
    "    else : \n",
    "        ## cross_val accuracies\n",
    "        accuracies = cross_val_score(model, X, Y, scoring='accuracy', cv=CV)\n",
    "        \n",
    "        fitted_model = model.fit(X_train,Y_train)\n",
    "        prediction_train = model.predict(X_train)\n",
    "        \n",
    "        ## compute KPIs\n",
    "        AUC = compute_AUC(model,X_test,Y_test)\n",
    "        Accuracy,Precision_df_1,Missclassified_1,TNR_df_1,TPR_df_1 = compute_KPIs(fitted_model,model_name,X_test, Y_test)\n",
    "    \n",
    "    \n",
    "    #### \n",
    "    #Creating DF with results for all models\n",
    "    ####\n",
    "    \n",
    "    Accuracy_df = pd.concat([Accuracy_df, Accuracy],axis = 1)\n",
    "    Precision_df = pd.concat([Precision_df, Precision_df_1],axis = 1)\n",
    "    Missclassified_1_df = pd.concat([Missclassified_1_df, Missclassified_1],axis = 1)\n",
    "    TNR_df = pd.concat([TNR_df, TNR_df_1],axis = 1)\n",
    "    TPR_df = pd.concat([TPR_df, TPR_df_1],axis = 1)\n",
    "    \n",
    "    ###\n",
    "    # Creating list for indiv KPIs /model\n",
    "    ###\n",
    "    \n",
    "    AUC_list.append(AUC)\n",
    "    \n",
    "    number_bads = sum(prediction_train)\n",
    "   \n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy,number_bads))\n",
    "        \n",
    "    fitted_models.append(fitted_model)\n",
    "    \n",
    "## removing redundant columns of indexes\n",
    "Accuracy_df= Accuracy_df.loc[:,~Accuracy_df.columns.duplicated()]\n",
    "Precision_df= Precision_df.loc[:,~Precision_df.columns.duplicated()]\n",
    "Missclassified_1_df= Missclassified_1_df.loc[:,~Missclassified_1_df.columns.duplicated()]\n",
    "TNR_df = TNR_df.loc[:,~TNR_df.columns.duplicated()]\n",
    "TPR_df = TPR_df.loc[:,~TPR_df.columns.duplicated()]\n",
    "\n",
    "print(\"\\n\",i, 'Model Tested')\n",
    "\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy','number_bads'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
